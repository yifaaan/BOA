import argparse
from pathlib import Path

def get_boa_arguments(**parser_kwargs):
    parser = argparse.ArgumentParser(**parser_kwargs)

    parser.add_argument("--cache_dir", type=str, default='cache')
    parser.add_argument("--print_memory_usage", action='store_true')
    
    ## Model
    parser.add_argument("--llm_path", type=str, default='facebook/opt-125m')
    parser.add_argument("--tokenizer_path", type=str, default=None)
    parser.add_argument("--eval_fp", action='store_true', help='Whether to evaluate the original fp model performance')
    
    ## Calib. Data
    parser.add_argument('--calib_data', type=str, default="wikitext2", choices=["c4", "wikitext2"])
    parser.add_argument('--nsamples', type=int, default=128, help='Number of calibration data samples.')
    parser.add_argument('--seqlen', type=int, default=2048, help='Length of input sequences')
    parser.add_argument('--seed', type=int, default=0, help='Seed for sampling the calibration data.')

    ## Quant. Configs.
    parser.add_argument('--w_bits', type=int, default=2)
    parser.add_argument('--w_sym', action="store_true")
    
    ## BoA Options
    parser.add_argument('--qparam_comput', type=str, default='Hessian', choices=['MinMax', 'MMSE', 'Hessian', 'AWQ'], help="How to determine Quant. Params")
    parser.add_argument('--block_v', action="store_true", help="Whether to apply block-wise objective for the value projection. In memory-limited cases, we can significantly reduce memory by de-activating this option, but at the expense of a slight performance degradation.")
    parser.add_argument('--act_order_col', action='store_true', help='Whether to reorder columns based on column-wise Hessian diagonals')
    parser.add_argument('--act_order_row', action='store_true', help='Whether to reorder rows based on row-wise Hessian diagonals')
    
    ## AWQ Options
    parser.add_argument('--use_awq', action='store_true', help='Enable AWQ-informed BOA (A-BOA) quantization')
    parser.add_argument('--awq_salient_ratio', type=float, default=0.01, help='Percentage of channels to protect (default: 1%)')
    parser.add_argument('--awq_alpha', type=float, default=0.5, help='AWQ smoothing parameter for scaling computation')

    parser.add_argument('--replace', type=float, default=1, help='Value to be replaced for the Hessian diagonal elements corresponding to dead neurons')
    
    # LM Eval Arguments
    parser.add_argument("--lm_eval", action="store_true", help="Evaluate the model on LM Eval tasks.")
    parser.add_argument('--tasks', nargs='+', default=["piqa", "hellaswag", "arc_easy", "arc_challenge", "winogrande", "lambada_openai", "lambada_standard", "openbookqa", "boolq"])
    parser.add_argument('--lm_eval_batch_size', type=int, default=16, help='Batch size for evaluating with lm eval harness.')
    
    args = parser.parse_args()

    Path(args.cache_dir).mkdir(parents=True, exist_ok=True)

    if args.tokenizer_path is None:
        args.tokenizer_path = args.llm_path
    args.llm_name = args.tokenizer_path.split('/')[-1]
    args.llm_type = args.llm_name.split('-')[0]

    args.replace = 1 / args.seqlen

    return args


def get_boa_weight_quant_infos(args):
    qconfigs = {
        "w_bits": args.w_bits,
        "w_sym": args.w_sym,
    }
    boa_opts = {
        "qparam_comput": args.qparam_comput,
        "block_v": args.block_v,
        'act_order_col': args.act_order_col, 
        'act_order_row': args.act_order_row,
        'use_awq': args.use_awq,
        'awq_salient_ratio': args.awq_salient_ratio,
        'awq_alpha': args.awq_alpha,
    }
    hyperparams = {"replace": args.replace}
    
    return qconfigs, boa_opts, hyperparams